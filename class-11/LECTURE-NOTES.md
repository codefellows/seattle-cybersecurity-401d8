# Lecture Notes: Foundational SIEM Operations

## Course Overview

Welcome to the Incident Response module. This week you'll learn how companies put together threat detection ecosystems using software like Splunk, Security Onion, and ELK Stack.

## Demo: Ops Challenge 26

Before we get started on today's Ops Challenge, we need to introduce certain concepts that will help you understand the objectives of today's challenge.

- **What**
  - What are Linux data streams?
    - Standard Input (stdin) accepts text as input
    - Standard Output (stdout) delivers text output
    - Standard Error (stderr) handles error messages generated by command
  - Note that streams are handled like files.
  - Pipes and redirects work on Linux data streams.

- **Why**
  - Why should we use Linux data streams?
    - Allow us to redirect system log files

## Lecture: Incident Response

- **Why** (5 min)

    1. Why incident response?
      - We need to see how data analysis and thread detection efforts correlate to the greater IR efforts of an org.
    1. Why automate data collection?
      1. Robust and repeatable process
      2. Supports lean security teams by reducing manual inspection
    1. Why use Splunk to automate collection?
      1. Splunk is a SIEM
      2. Aggregate systems logs in real time

- **What** (10 min)

  - What is an "incident"?
    - An event involving the breach of a system's security policy in order to affect its integrity or availability and/or the unauthorized access or attempted access to a system or systems

  - What is "incident response"? [Incident](https://digitalguardian.com/blog/what-incident-response)
    - Process by which an organization handles a data breach or cyberattack, including the way the organization attempts to manage the consequences of the attack or breach (the “incident”)
    - A computer incident response team (CIRT) exists to enact playbooks against incidents.
    - What is the NIST incident response lifecycle?

    ![incident](assets/incident-response.png)

      1. NIST SP800-61: The Incident Response Lifecycle
        1. Preparation
        2. Detection & Analysis
        3. Containment, Eradication, and Recovery
        4. Post-Incident Activities

  - Data collection can and should be automated. What levels of data collection automation are there?
    - SQRRL automation maturity model

    ![sqrrl](assets/sqrrl.png)

  - What is a SIEM? [SIEM](https://www.tripwire.com/state-of-security/incident-detection/log-management-siem/what-is-a-siem/)
    - Security information and event management software
      - A “SIEM” is defined as a group of complex technologies that together provide a bird’s-eye view into an infrastructure.
    - Performs these core functions for a SOC:
      - Event and log collection
      - Layered centric views or heterogeneous
      - Normalization
      - Correlation
      - Adaptability (scalable)
      - Reporting and alerting
      - Log management
    - Key takeaways
      - A “SIEM” is defined as a group of complex technologies that together provide a bird’s-eye view into an infrastructure.
      - Provides centralized security event management
      - Provides correlation and normalization for context and alerting
      - Provides reporting on all ingested data
      - Can take in data from virtually any vendor or in-house applications

## Lecture: Splunk Architecture & Planning Data Sources

> SIEMs are capable of aggregating multiple data sources into one viewable dashboard of valuable information.

- **Why**
  - Why do we need to add data sources to Splunk?
    - We want to view the organizations aggregate data from a single place.
    - In this way, a SIEM adds value to an org's operations by reducing time to get answers.
    - Avoid leaving logs stored on single computers

- **What**
  - What are the stages of the SIEM data pipeline [SIEM](https://mindmajix.com/overview-of-splunk-architecture)?
    - STAGE 1: Data Input
      - In this stage, all the data will be accessed from the source. The metadata keys include the following:
        - Hostname
        - Source
        - Source type of data
    - STAGE 2: Data Storage
      - This stage is carried out in two phases:
        - Parsing
          - In the **parsing** phase, the Splunk software examines, analyzes, and transforms the data. This phase is called event processing where all the data sets are broken down into different events. The following activities happen within this parsing phase.
            - Stream of data is broken down into individual lines
            - Identifies and sets time stamps
            - Transforms the metadata and events according to regex standards.
        - Indexing
          - In the **indexing** phase, the Splunk software writes parsed events to the index queue. The main benefit of using this is to make sure the data is easily available for anyone at the time of the search.
          - Individual **indexes** are created as objects in Splunk, and you define what scope of data gets monitored and goes into each index.
    - STAGE 3: Data Searching
      - How data is accessed, used, and viewed is controlled.
      - Splunk will store user-defined knowledge objects like reports, event types, and alerts.

  - What are the basic components of Splunk?
    - Splunk Forwarder: Forward the data
      - Splunk Universal Forwarder
      - Splunk Heavy Forwarder
    - Splunk Indexer: Parsing data and Indexing the data
      - Splunk Indexer tool helps the data to be converted into events and indexed so that it is easy for performing search operations efficiently.
      - Data from forwarders gets parsed (incl. unwanted data removed) then index it.
    - Search Head: User interface where the user will have an option to search, analyze and report data.
      - Search head: User interface where data is retrieved by keyword
      - Search peer: Search results & indexing

  - What data sources does Splunk support?
    - Many! Includes old-fashioned syslog.

  - What data sources will Splunk typically be setup with?
    - Enterprise with multiple Windows Servers will forward their logs to Splunk

  - What ingestion techniques does Splunk support?
    - Syslog via Sysmon
    - Splunk Universal Data Forwarder

  - What are some common challenges/problems encountered with ingesting logs from many sources?
    - Inconsistent time/date formatting
    - Syntax issues
